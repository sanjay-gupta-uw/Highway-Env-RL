{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and make sure highway-env is installed properly\n",
    "import gymnasium\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment with visual rendering\n",
    "env = gymnasium.make(\"highway-better-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Render and show the first frame\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.title(\"Initial Frame\")\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# print the environment information\n",
    "print(\"Environment Information:\")\n",
    "pprint(env.unwrapped.config)\n",
    "\n",
    "import tensorboard\n",
    "print(tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d70958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# === Create wrapped evaluation env ===\n",
    "def make_env(str_env=None):\n",
    "    def _init():\n",
    "        if str_env is None or str_env == \"highway\":\n",
    "            env_id = \"highway-better-v1\"\n",
    "        elif str_env == \"intersection\":\n",
    "            env_id = \"intersection-v1\"\n",
    "        elif str_env == \"racetrack\":\n",
    "            env_id = \"racetrack-v0\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown environment: {str_env}\")\n",
    "\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        return Monitor(env)\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14926ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# === Global reward tracker ===\n",
    "best_rewards = {}\n",
    "\n",
    "# === Optuna Callback for pruning ===\n",
    "class OptunaCallback(BaseCallback):\n",
    "    def __init__(self, trial, eval_freq=2000, n_eval_episodes=3, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.trial = trial\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            reward, _ = evaluate_policy(self.model, self.training_env, n_eval_episodes=self.n_eval_episodes, deterministic=True)\n",
    "            self.trial.report(reward, self.n_calls)\n",
    "            if self.trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        return True\n",
    "\n",
    "# === Objective Function for PPO ===\n",
    "def objective(trial, phase, str_env, coarse_params=None, save_dir=None):\n",
    "    global best_rewards\n",
    "    coarse_params_path = os.path.join(save_dir, \"PPO_best_coarse_params.json\")\n",
    "\n",
    "    # Init per-env reward tracking\n",
    "    if str_env not in best_rewards:\n",
    "        best_rewards[str_env] = {\"coarse\": -float(\"inf\"), \"fine\": -float(\"inf\")}\n",
    "\n",
    "    # === Coarse Phase ===\n",
    "    if phase == \"coarse\":\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.85, 0.999)\n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", ([64, 64], [128, 128], [256, 256]))\n",
    "\n",
    "        config = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"gamma\": gamma,\n",
    "            \"net_arch\": net_arch,\n",
    "        }\n",
    "\n",
    "    # === Fine Phase ===\n",
    "    elif phase == \"fine\":\n",
    "        assert coarse_params is not None, \"Need coarse params for fine tuning\"\n",
    "\n",
    "        config = {\n",
    "            **coarse_params,\n",
    "            \"entropy_coef\": trial.suggest_float(\"entropy_coef\", 1e-4, 0.05),\n",
    "            \"clip_range\": trial.suggest_float(\"clip_range\", 0.1, 0.3),\n",
    "            \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.8, 0.99),\n",
    "            \"vf_coef\": trial.suggest_float(\"vf_coef\", 0.3, 0.9),\n",
    "            \"max_grad_norm\": trial.suggest_float(\"max_grad_norm\", 0.3, 1.0),\n",
    "            \"n_steps\": trial.suggest_categorical(\"n_steps\", [64, 128, 256]),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        }\n",
    "\n",
    "    # === Build Environment and Model ===\n",
    "    env = make_vec_env(make_env(str_env), n_envs=1)\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        policy_kwargs={\"net_arch\": config[\"net_arch\"]},\n",
    "        ent_coef=config.get(\"entropy_coef\", 0.01),\n",
    "        clip_range=config.get(\"clip_range\", 0.2),\n",
    "        gae_lambda=config.get(\"gae_lambda\", 0.95),\n",
    "        vf_coef=config.get(\"vf_coef\", 0.5),\n",
    "        max_grad_norm=config.get(\"max_grad_norm\", 0.5),\n",
    "        n_steps=config.get(\"n_steps\", 128),\n",
    "        batch_size=config.get(\"batch_size\", 64),\n",
    "        verbose=0,\n",
    "        tensorboard_log=f\"../tensorboard_logs/{str_env}/PPO_phase_{phase}\",\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=10_000, callback=OptunaCallback(trial))\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "    env.close()\n",
    "    trial.set_user_attr(\"mean_reward\", mean_reward)\n",
    "\n",
    "    # === Save Best Model If Improved ===\n",
    "    if mean_reward > best_rewards[str_env][phase]:\n",
    "        best_rewards[str_env][phase] = mean_reward\n",
    "        model.save(os.path.join(save_dir, f\"PPO_best_{phase}.zip\"))\n",
    "        print(f\"ðŸ’¾ Saved new best {phase} model (trial {trial.number}) for {str_env}\")\n",
    "        if phase == \"coarse\":\n",
    "            with open(coarse_params_path, \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            print(f\"âœ… Coarse tuning params saved for {str_env} (trial {trial.number})\")\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# === COARSE PHASE ===\n",
    "def run_coarse_phase(str_env):\n",
    "    print(f\"ðŸ”§ Starting COARSE tuning for {str_env}...\")\n",
    "    save_dir = f\"../trained_models/{str_env}/PPO/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=1)\n",
    "    )\n",
    "    study.optimize(lambda trial: objective(trial, phase=\"coarse\", str_env=str_env, save_dir=save_dir), n_trials=15)\n",
    "\n",
    "# === FINE PHASE ===\n",
    "def run_fine_phase(str_env):\n",
    "    save_dir = f\"../trained_models/{str_env}/PPO/\"\n",
    "    coarse_params_path = os.path.join(save_dir, \"PPO_best_coarse_params.json\")\n",
    "\n",
    "    if not os.path.exists(coarse_params_path):\n",
    "        raise FileNotFoundError(f\"Missing coarse phase results for {str_env}. Run coarse phase first.\")\n",
    "    with open(coarse_params_path, \"r\") as f:\n",
    "        coarse_params = json.load(f)\n",
    "\n",
    "    print(f\"ðŸ”¬ Starting FINE tuning for {str_env}...\")\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=123),\n",
    "        pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=1)\n",
    "    )\n",
    "    study.optimize(lambda trial: objective(trial, phase=\"fine\", str_env=str_env, coarse_params=coarse_params, save_dir=save_dir), n_trials=15)\n",
    "    print(f\"âœ… Fine tuning complete for {str_env}.\")\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "run_coarse = True\n",
    "run_fine = True\n",
    "\n",
    "env_list = [\"highway\", \"intersection\", \"racetrack\"]\n",
    "for str_env in env_list:\n",
    "    print(f\"\\nðŸš¦ Running PPO tuning for environment: {str_env}\")\n",
    "    if run_coarse:\n",
    "        run_coarse_phase(str_env)\n",
    "    if run_fine:\n",
    "        run_fine_phase(str_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# import gymnasium as gym\n",
    "\n",
    "# SAVE_DIR = \"../trained_models/highway/PPO/\"\n",
    "# # === Load trained Optuna model ===\n",
    "# model_path = os.path.join(SAVE_DIR, \"PPO_best_fine.zip\")\n",
    "# model = PPO.load(model_path)\n",
    "\n",
    "# # === Environment for continued training ===\n",
    "# def make_env():\n",
    "#     env = gym.make(\"highway-better-v1\")\n",
    "#     return Monitor(env)\n",
    "\n",
    "# train_env = make_vec_env(make_env, n_envs=1)\n",
    "\n",
    "# # === Rebind environment in case original wasn't saved in model ===\n",
    "# model.set_env(train_env)\n",
    "\n",
    "# # === Training configuration ===\n",
    "# total_timesteps = 40000\n",
    "# save_interval = 10000\n",
    "# timesteps_run = 0\n",
    "\n",
    "# cp_log_dir = f\"../checkpoints/highway/PPO_trained_model_tuned\"\n",
    "# os.makedirs(cp_log_dir, exist_ok=True)\n",
    "\n",
    "# while timesteps_run < total_timesteps:\n",
    "#     model.learn(\n",
    "#         total_timesteps=save_interval,\n",
    "#         reset_num_timesteps=False,\n",
    "#         tb_log_name=\"highway_PPO_tuned\",\n",
    "#         log_interval=1,\n",
    "#     )\n",
    "#     timesteps_run += save_interval\n",
    "#     model.save(f\"{cp_log_dir}/{timesteps_run}\")\n",
    "#     print(f\"âœ… Saved checkpoint at {timesteps_run} timesteps\")\n",
    "\n",
    "# # === Save final model ===\n",
    "# final_model_path = os.path.join(SAVE_DIR, \"PPO_trained_tuned.zip\")\n",
    "# model.save(final_model_path)\n",
    "# print(f\"âœ… Final model saved at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8acbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import imageio\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from IPython.display import Video\n",
    "\n",
    "# # === Configuration ===\n",
    "# algo_name = \"PPO\"\n",
    "# video_eval_dir = f\"../tuned_videos/highway/{algo_name}_tuned/video_eval\"\n",
    "# os.makedirs(video_eval_dir, exist_ok=True)\n",
    "# video_path = os.path.join(video_eval_dir, f\"{algo_name}_eval.mp4\")\n",
    "\n",
    "# # === Load trained model ===\n",
    "# model_path = \"../trained_models/highway/PPO/PPO_trained_tuned.zip\"\n",
    "# model = PPO.load(model_path)\n",
    "\n",
    "# env = make_vec_env(make_env(render_mode=\"rgb_array\"), n_envs=1)\n",
    "# # === Evaluate and collect frames ===\n",
    "# frames = []\n",
    "# num_episodes = 5  # Number of episodes to evaluate\n",
    "\n",
    "# for i in range(num_episodes):\n",
    "#     # === Synchronize the two environments ===\n",
    "#     obs_stacked = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs_stacked)\n",
    "#         obs_stacked, _, done, _ = env.step(action)\n",
    "\n",
    "#         frame = env.render()\n",
    "#         frames.append(frame)\n",
    "#         if done[0]:  \n",
    "#             break\n",
    "\n",
    "#     # Add a few idle frames for padding\n",
    "#     for _ in range(10):\n",
    "#         frames.append(frames[-1])\n",
    "\n",
    "# # === Save video ===\n",
    "# imageio.mimsave(video_path, frames, fps=30)\n",
    "\n",
    "# # === Display video ===\n",
    "# Video(video_path, embed=True, width=600, height=400)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
