{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ac9b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries and make sure highway-env is installed properly\n",
    "import gymnasium\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment with visual rendering\n",
    "env = gymnasium.make(\"highway-better-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Render and show the first frame\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.title(\"Initial Frame\")\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5e5fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Information:\n",
      "{'action': {'acceleration_range': [-1, 1],\n",
      "            'lateral': True,\n",
      "            'longitudinal': True,\n",
      "            'speed_range': [0, 40],\n",
      "            'steering_range': [-0.1, 0.1],\n",
      "            'type': 'ContinuousAction'},\n",
      " 'centering_position': [0.3, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 60,\n",
      " 'ego_spacing': 1.5,\n",
      " 'high_speed_reward': 0.4,\n",
      " 'initial_lane_id': None,\n",
      " 'lane_change_reward': 0,\n",
      " 'lanes_count': 5,\n",
      " 'manual_control': False,\n",
      " 'normalize_reward': True,\n",
      " 'observation': {'absolute': False,\n",
      "                 'features': ['presence',\n",
      "                              'x',\n",
      "                              'y',\n",
      "                              'vx',\n",
      "                              'vy',\n",
      "                              'heading',\n",
      "                              'lat_off'],\n",
      "                 'type': 'Kinematics',\n",
      "                 'vehicles_count': 40},\n",
      " 'offroad_terminal': True,\n",
      " 'offscreen_rendering': False,\n",
      " 'on_road_reward': 0.5,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 1,\n",
      " 'real_time_rendering': True,\n",
      " 'render_agent': True,\n",
      " 'reward_heading_deviation': 0.5,\n",
      " 'reward_speed_range': [20, 40],\n",
      " 'right_lane_reward': 0,\n",
      " 'scaling': 5.5,\n",
      " 'screen_height': 150,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 5,\n",
      " 'vehicles_count': 20,\n",
      " 'vehicles_density': 1}\n",
      "2.17.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# print the environment information\n",
    "print(\"Environment Information:\")\n",
    "pprint(env.unwrapped.config)\n",
    "\n",
    "import torch\n",
    "import tensorboard\n",
    "\n",
    "print(tensorboard.__version__)\n",
    "print(torch.cuda.is_available())   \n",
    "# print(torch.cuda.get_device_name(0))  # Should show GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0037297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# === Create wrapped evaluation env ===\n",
    "def make_env(str_env=None):\n",
    "    def _init():\n",
    "        if str_env is None or str_env == \"highway\":\n",
    "            env_id = \"highway-better-v1\"\n",
    "        elif str_env == \"intersection\":\n",
    "            env_id = \"intersection-v1\"\n",
    "        elif str_env == \"racetrack\":\n",
    "            env_id = \"racetrack-v0\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown environment: {str_env}\")\n",
    "\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        return Monitor(env)\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1de8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([make_env(\"highway\")])  \n",
    "# print the action space\n",
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 11:14:51,619] A new study created in memory with name: SAC_fine_intersection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¦ Running SAC tuning for environment: intersection\n",
      "ðŸ”¬ Starting FINE tuning for intersection...\n",
      "ðŸ”§ Training fine model for intersection with config: {'learning_rate': 0.0008971270805430311, 'gamma': 0.9553262389274892, 'net_arch': [128, 128], 'learning_starts': 1000, 'batch_size': 32, 'tau': 0.013269721536243367, 'train_freq': 16, 'gradient_steps': 1, 'ent_coef': 'auto_0.1', 'use_sde': False}\n",
      "ðŸ“Š Trial 0 | Step 500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 1000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 1500 | Reward: 1.23\n",
      "ðŸ“Š Trial 0 | Step 2000 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 2500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 3000 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 3500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 4000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 4500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 5000 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 5500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 6000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 6500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 7000 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 7500 | Reward: 0.73\n",
      "ðŸ“Š Trial 0 | Step 8000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 8500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 9000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 9500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 10000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 10500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 11000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 11500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 12000 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 12500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 13000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 13500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 14000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 14500 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 15000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 15500 | Reward: 2.00\n",
      "ðŸ“Š Trial 0 | Step 16000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 16500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 17000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 17500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 18000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 18500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 19000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 19500 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 20000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 20500 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 21000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 21500 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 22000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 22500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 23000 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 23500 | Reward: 0.00\n",
      "ðŸ“Š Trial 0 | Step 24000 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 24500 | Reward: 1.00\n",
      "ðŸ“Š Trial 0 | Step 25000 | Reward: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 11:56:59,120] Trial 0 finished with value: 1.0 and parameters: {'batch_size': 32, 'tau': 0.013269721536243367, 'train_freq': 16, 'gradient_steps': 1, 'ent_coef': 'auto_0.1', 'use_sde': False}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved new best fine model (trial 0) for intersection\n",
      "ðŸ”§ Training fine model for intersection with config: {'learning_rate': 0.0008971270805430311, 'gamma': 0.9553262389274892, 'net_arch': [128, 128], 'learning_starts': 1000, 'batch_size': 64, 'tau': 0.0076317763422123885, 'train_freq': 16, 'gradient_steps': 1, 'ent_coef': 'auto', 'use_sde': False}\n",
      "ðŸ“Š Trial 1 | Step 500 | Reward: 0.00\n",
      "ðŸ“Š Trial 1 | Step 1000 | Reward: 1.00\n",
      "ðŸ“Š Trial 1 | Step 1500 | Reward: -4.00\n",
      "ðŸ“Š Trial 1 | Step 2000 | Reward: 1.00\n",
      "ðŸ“Š Trial 1 | Step 2500 | Reward: 1.73\n",
      "ðŸ“Š Trial 1 | Step 3000 | Reward: 1.86\n",
      "ðŸ“Š Trial 1 | Step 3500 | Reward: 0.00\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "torch.set_num_threads(1)  # Prevent CPU thread oversubscription\n",
    "best_rewards = {}\n",
    "\n",
    "# === Callback for pruning ===\n",
    "class OptunaCallback(BaseCallback):\n",
    "    def __init__(self, trial, eval_freq=500, n_eval_episodes=1, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.trial = trial\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.last_reward = -float(\"inf\")\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            try:\n",
    "                self.last_reward, _ = evaluate_policy(\n",
    "                    self.model,\n",
    "                    self.training_env,\n",
    "                    n_eval_episodes=self.n_eval_episodes,\n",
    "                    deterministic=True\n",
    "                )\n",
    "                print(f\"ðŸ“Š Trial {self.trial.number} | Step {self.n_calls} | Reward: {self.last_reward:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Evaluation failed at step {self.n_calls}: {e}\")\n",
    "                self.last_reward = -float(\"inf\")\n",
    "\n",
    "            self.trial.report(self.last_reward, self.n_calls)\n",
    "            if self.trial.should_prune():\n",
    "                print(f\"â¹ï¸ Trial {self.trial.number} pruned at step {self.n_calls}\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        return True  # âœ… Must return True to continue training\n",
    "\n",
    "\n",
    "# === Objective Function ===\n",
    "def objective(trial, phase, str_env, coarse_params=None, save_dir=None):\n",
    "    global best_rewards\n",
    "    coarse_params_path = os.path.join(save_dir, \"SAC_best_coarse_params.json\")\n",
    "\n",
    "    if str_env not in best_rewards:\n",
    "        best_rewards[str_env] = {\"coarse\": -float(\"inf\"), \"fine\": -float(\"inf\")}\n",
    "\n",
    "    # === Hyperparameters ===\n",
    "    if phase == \"coarse\":\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [[64, 64], [128, 128], [256, 256]])\n",
    "        learning_starts = trial.suggest_categorical(\"learning_starts\", [1000, 2000])\n",
    "        config = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"gamma\": gamma,\n",
    "            \"net_arch\": net_arch,\n",
    "            \"learning_starts\": learning_starts,\n",
    "        }\n",
    "    elif phase == \"fine\":\n",
    "        assert coarse_params is not None\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        tau = trial.suggest_float(\"tau\", 0.005, 0.02)\n",
    "        train_freq = trial.suggest_categorical(\"train_freq\", [1, 8, 16])\n",
    "        gradient_steps = trial.suggest_categorical(\"gradient_steps\", [1, 4, 8])\n",
    "        ent_coef = trial.suggest_categorical(\"ent_coef\", [\"auto\", \"auto_0.1\"])\n",
    "        use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "        config = {**coarse_params, \"batch_size\": batch_size, \"tau\": tau,\n",
    "                  \"train_freq\": train_freq, \"gradient_steps\": gradient_steps,\n",
    "                  \"ent_coef\": ent_coef, \"use_sde\": use_sde}\n",
    "\n",
    "    n_envs = 1\n",
    "    env = DummyVecEnv([make_env(str_env) for _ in range(n_envs)])\n",
    "\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        policy_kwargs={\"net_arch\": config[\"net_arch\"]},\n",
    "        batch_size=config.get(\"batch_size\", 64),\n",
    "        tau=config.get(\"tau\", 0.005),\n",
    "        train_freq=config.get(\"train_freq\", 1),\n",
    "        gradient_steps=config.get(\"gradient_steps\", 1),\n",
    "        ent_coef=config.get(\"ent_coef\", \"auto\"),\n",
    "        learning_starts=config.get(\"learning_starts\", 1000),\n",
    "        use_sde=config.get(\"use_sde\", False),\n",
    "        verbose=0,\n",
    "        tensorboard_log=f\"../tensorboard_logs/{str_env}_copy/SAC_phase_{phase}\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    total_steps = 25_000\n",
    "    callback = OptunaCallback(trial)\n",
    "    print(f\"ðŸ”§ Training {phase} model for {str_env} with config: {config}\")\n",
    "    model.learn(total_timesteps=total_steps, callback=callback)\n",
    "\n",
    "    mean_reward = callback.last_reward\n",
    "    trial.set_user_attr(\"mean_reward\", mean_reward)\n",
    "\n",
    "    if mean_reward > best_rewards[str_env][phase]:\n",
    "        best_rewards[str_env][phase] = mean_reward\n",
    "        model.save(os.path.join(save_dir, f\"SAC_best_{phase}.zip\"))\n",
    "        print(f\"ðŸ’¾ Saved new best {phase} model (trial {trial.number}) for {str_env}\")\n",
    "        if phase == \"coarse\":\n",
    "            with open(coarse_params_path, \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            print(f\"âœ… Coarse tuning params saved for {str_env} (trial {trial.number})\")\n",
    "\n",
    "    env.close()\n",
    "    # if hasattr(env, \"close_extras\"):\n",
    "    #     env.close_extras()  # Closes SubprocVecEnv child processes\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# === PHASE FUNCTIONS ===\n",
    "def run_coarse_phase(str_env):\n",
    "    print(f\"ðŸ”§ Starting COARSE tuning for {str_env}...\")\n",
    "    save_dir = f\"../trained_models/{str_env}_copy/SAC/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"SAC_coarse_{str_env}\",\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, phase=\"coarse\", str_env=str_env, save_dir=save_dir),\n",
    "        n_trials=10,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "def run_fine_phase(str_env):\n",
    "    save_dir = f\"../trained_models/{str_env}_copy/SAC/\"\n",
    "    coarse_params_path = os.path.join(save_dir, \"SAC_best_coarse_params.json\")\n",
    "\n",
    "    if not os.path.exists(coarse_params_path):\n",
    "        raise FileNotFoundError(f\"Missing coarse phase results for {str_env}. Run coarse phase first.\")\n",
    "\n",
    "    with open(coarse_params_path, \"r\") as f:\n",
    "        coarse_params = json.load(f)\n",
    "\n",
    "    print(f\"ðŸ”¬ Starting FINE tuning for {str_env}...\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"SAC_fine_{str_env}\",\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=123),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, phase=\"fine\", str_env=str_env, coarse_params=coarse_params, save_dir=save_dir),\n",
    "        n_trials=10,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    with open(os.path.join(save_dir, f\"optuna_study_fine.json\"), \"w\") as f:\n",
    "        f.write(study.trials_dataframe().to_json(orient=\"records\", indent=2))\n",
    "\n",
    "\n",
    "run_coarse = True\n",
    "run_fine = True\n",
    "env_list = [\"racetrack\"]\n",
    "\n",
    "for str_env in env_list:\n",
    "    print(f\"\\nðŸš¦ Running SAC tuning for environment: {str_env}\")\n",
    "    if run_coarse:\n",
    "        run_coarse_phase(str_env)\n",
    "    if run_fine:\n",
    "        run_fine_phase(str_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from stable_baselines3 import SAC\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# import gymnasium as gym\n",
    "\n",
    "# SAVE_DIR = \"../trained_models/highway/SAC/\"\n",
    "# # === Load trained Optuna model ===\n",
    "# model_path = os.path.join(SAVE_DIR, \"SAC_best_fine.zip\")\n",
    "# model = SAC.load(model_path)\n",
    "\n",
    "# # === Environment for continued training ===\n",
    "# def make_env(render_mode=None):\n",
    "#     def _init():\n",
    "#         config = {\n",
    "#             \"action\": {\n",
    "#                 \"type\": \"ContinuousAction\"\n",
    "#             },\n",
    "#         }\n",
    "#         env = gym.make(\"highway-fast-v0\", render_mode=render_mode, config=config)\n",
    "#         return Monitor(env)\n",
    "#     return _init\n",
    "\n",
    "\n",
    "# train_env = make_vec_env(make_env(), n_envs=1)\n",
    "\n",
    "# # === Rebind environment in case original wasn't saved in model ===\n",
    "# model.set_env(train_env)\n",
    "\n",
    "# # === Training configuration ===\n",
    "# total_timesteps = 40000\n",
    "# save_interval = 10000\n",
    "# timesteps_run = 0\n",
    "\n",
    "# cp_log_dir = f\"../checkpoints/highway/SAC_optuna\"\n",
    "# os.makedirs(cp_log_dir, exist_ok=True)\n",
    "\n",
    "# while timesteps_run < total_timesteps:\n",
    "#     model.learn(\n",
    "#         total_timesteps=save_interval,\n",
    "#         reset_num_timesteps=False,\n",
    "#         tb_log_name=\"highway_SAC_optuna\",\n",
    "#         log_interval=1,\n",
    "#     )\n",
    "#     timesteps_run += save_interval\n",
    "#     model.save(f\"{cp_log_dir}/{timesteps_run}\")\n",
    "#     print(f\"âœ… Saved checkpoint at {timesteps_run} timesteps\")\n",
    "\n",
    "# SAVE_DIR = \"../trained_models/highway/SAC/trained_model_tuned\"\n",
    "# # === Save final model ===\n",
    "# final_model_path = SAVE_DIR\n",
    "# model.save(final_model_path)\n",
    "# print(f\"âœ… Final model saved at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import imageio\n",
    "# from stable_baselines3 import SAC\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# import gymnasium as gym\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from IPython.display import Video\n",
    "\n",
    "# # === Configuration ===\n",
    "# algo_name = \"SAC\"\n",
    "# video_eval_dir = f\"../tuned/highway/{algo_name}_tuned/video_eval\"\n",
    "# os.makedirs(video_eval_dir, exist_ok=True)\n",
    "# video_path = os.path.join(video_eval_dir, f\"{algo_name}_eval.mp4\")\n",
    "\n",
    "# # === Load trained model ===\n",
    "# model_path = \"../trained_models/highway/SAC/trained_model_tuned.zip\"\n",
    "# model = SAC.load(model_path)\n",
    "\n",
    "# # === Create vectorized, stacked env for model input ===\n",
    "# def make_env(render_mode=None):\n",
    "#     def _init():\n",
    "#         config = {\n",
    "#             \"action\": {\n",
    "#                 \"type\": \"ContinuousAction\"\n",
    "#             },\n",
    "#         }\n",
    "#         env = gym.make(\"highway-fast-v0\", render_mode=render_mode, config=config)\n",
    "#         return Monitor(env)\n",
    "#     return _init\n",
    "\n",
    "\n",
    "# env = make_vec_env(make_env(render_mode=\"rgb_array\"), n_envs=1)\n",
    "# # === Evaluate and collect frames ===\n",
    "# frames = []\n",
    "# num_episodes = 5  # Number of episodes to evaluate\n",
    "\n",
    "# for i in range(num_episodes):\n",
    "#     # === Synchronize the two environments ===\n",
    "#     obs_stacked = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs_stacked)\n",
    "#         obs_stacked, _, done, _ = env.step(action)\n",
    "\n",
    "#         frame = env.render()\n",
    "#         frames.append(frame)\n",
    "#         if done[0]:  # SAC + VecEnv = done is a list\n",
    "#             break\n",
    "\n",
    "#     # Add a few idle frames for padding\n",
    "#     for _ in range(10):\n",
    "#         frames.append(frames[-1])\n",
    "\n",
    "# # === Save video ===\n",
    "# imageio.mimsave(video_path, frames, fps=30)\n",
    "\n",
    "# # === Display video ===\n",
    "# Video(video_path, embed=True, width=600, height=400)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-sac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
