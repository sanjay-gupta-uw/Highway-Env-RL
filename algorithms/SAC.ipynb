{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and make sure highway-env is installed properly\n",
    "import gymnasium\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the environment with visual rendering\n",
    "env = gymnasium.make(\"highway-better-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Render and show the first frame\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.title(\"Initial Frame\")\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# print the environment information\n",
    "print(\"Environment Information:\")\n",
    "pprint(env.unwrapped.config)\n",
    "\n",
    "import torch\n",
    "import tensorboard\n",
    "\n",
    "print(tensorboard.__version__)\n",
    "print(torch.cuda.is_available())   \n",
    "# print(torch.cuda.get_device_name(0))  # Should show GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0037297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# === Create wrapped evaluation env ===\n",
    "def make_env(str_env=None):\n",
    "    def _init():\n",
    "        if str_env is None or str_env == \"highway\":\n",
    "            env_id = \"highway-better-v1\"\n",
    "        elif str_env == \"intersection\":\n",
    "            env_id = \"intersection-v1\"\n",
    "        elif str_env == \"racetrack\":\n",
    "            env_id = \"racetrack-v0\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown environment: {str_env}\")\n",
    "\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        return Monitor(env)\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1de8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([make_env(\"highway\")])  \n",
    "# print the action space\n",
    "print(\"Action Space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "torch.set_num_threads(1)  # Prevent CPU thread oversubscription\n",
    "best_rewards = {}\n",
    "\n",
    "# === Callback for pruning ===\n",
    "class OptunaCallback(BaseCallback):\n",
    "    def __init__(self, trial, eval_freq=500, n_eval_episodes=1, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.trial = trial\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.last_reward = -float(\"inf\")\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            try:\n",
    "                self.last_reward, _ = evaluate_policy(\n",
    "                    self.model,\n",
    "                    self.training_env,\n",
    "                    n_eval_episodes=self.n_eval_episodes,\n",
    "                    deterministic=True\n",
    "                )\n",
    "                print(f\"📊 Trial {self.trial.number} | Step {self.n_calls} | Reward: {self.last_reward:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Evaluation failed at step {self.n_calls}: {e}\")\n",
    "                self.last_reward = -float(\"inf\")\n",
    "\n",
    "            self.trial.report(self.last_reward, self.n_calls)\n",
    "            if self.trial.should_prune():\n",
    "                print(f\"⏹️ Trial {self.trial.number} pruned at step {self.n_calls}\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        return True  # ✅ Must return True to continue training\n",
    "\n",
    "\n",
    "# === Objective Function ===\n",
    "def objective(trial, phase, str_env, coarse_params=None, save_dir=None):\n",
    "    global best_rewards\n",
    "    coarse_params_path = os.path.join(save_dir, \"SAC_best_coarse_params.json\")\n",
    "\n",
    "    if str_env not in best_rewards:\n",
    "        best_rewards[str_env] = {\"coarse\": -float(\"inf\"), \"fine\": -float(\"inf\")}\n",
    "\n",
    "    # === Hyperparameters ===\n",
    "    if phase == \"coarse\":\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [[64, 64], [128, 128], [256, 256]])\n",
    "        learning_starts = trial.suggest_categorical(\"learning_starts\", [1000, 2000])\n",
    "        config = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"gamma\": gamma,\n",
    "            \"net_arch\": net_arch,\n",
    "            \"learning_starts\": learning_starts,\n",
    "        }\n",
    "    elif phase == \"fine\":\n",
    "        assert coarse_params is not None\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        tau = trial.suggest_float(\"tau\", 0.005, 0.02)\n",
    "        train_freq = trial.suggest_categorical(\"train_freq\", [1, 8, 16])\n",
    "        gradient_steps = trial.suggest_categorical(\"gradient_steps\", [1, 4, 8])\n",
    "        ent_coef = trial.suggest_categorical(\"ent_coef\", [\"auto\", \"auto_0.1\"])\n",
    "        use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "        config = {**coarse_params, \"batch_size\": batch_size, \"tau\": tau,\n",
    "                  \"train_freq\": train_freq, \"gradient_steps\": gradient_steps,\n",
    "                  \"ent_coef\": ent_coef, \"use_sde\": use_sde}\n",
    "\n",
    "    n_envs = 1\n",
    "    env = DummyVecEnv([make_env(str_env) for _ in range(n_envs)])\n",
    "\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        policy_kwargs={\"net_arch\": config[\"net_arch\"]},\n",
    "        batch_size=config.get(\"batch_size\", 64),\n",
    "        tau=config.get(\"tau\", 0.005),\n",
    "        train_freq=config.get(\"train_freq\", 1),\n",
    "        gradient_steps=config.get(\"gradient_steps\", 1),\n",
    "        ent_coef=config.get(\"ent_coef\", \"auto\"),\n",
    "        learning_starts=config.get(\"learning_starts\", 1000),\n",
    "        use_sde=config.get(\"use_sde\", False),\n",
    "        verbose=0,\n",
    "        tensorboard_log=f\"../tensorboard_logs/{str_env}/SAC_phase_{phase}\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    total_steps = 25_000\n",
    "    callback = OptunaCallback(trial)\n",
    "    print(f\"🔧 Training {phase} model for {str_env} with config: {config}\")\n",
    "    model.learn(total_timesteps=total_steps, callback=callback)\n",
    "\n",
    "    mean_reward = callback.last_reward\n",
    "    trial.set_user_attr(\"mean_reward\", mean_reward)\n",
    "\n",
    "    if mean_reward > best_rewards[str_env][phase]:\n",
    "        best_rewards[str_env][phase] = mean_reward\n",
    "        model.save(os.path.join(save_dir, f\"SAC_best_{phase}.zip\"))\n",
    "        print(f\"💾 Saved new best {phase} model (trial {trial.number}) for {str_env}\")\n",
    "        if phase == \"coarse\":\n",
    "            with open(coarse_params_path, \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            print(f\"✅ Coarse tuning params saved for {str_env} (trial {trial.number})\")\n",
    "\n",
    "    env.close()\n",
    "    # if hasattr(env, \"close_extras\"):\n",
    "    #     env.close_extras()  # Closes SubprocVecEnv child processes\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# === PHASE FUNCTIONS ===\n",
    "def run_coarse_phase(str_env):\n",
    "    print(f\"🔧 Starting COARSE tuning for {str_env}...\")\n",
    "    save_dir = f\"../trained_models/{str_env}/SAC/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"SAC_coarse_{str_env}\",\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, phase=\"coarse\", str_env=str_env, save_dir=save_dir),\n",
    "        n_trials=10,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "def run_fine_phase(str_env):\n",
    "    save_dir = f\"../trained_models/{str_env}/SAC/\"\n",
    "    coarse_params_path = os.path.join(save_dir, \"SAC_best_coarse_params.json\")\n",
    "\n",
    "    if not os.path.exists(coarse_params_path):\n",
    "        raise FileNotFoundError(f\"Missing coarse phase results for {str_env}. Run coarse phase first.\")\n",
    "\n",
    "    with open(coarse_params_path, \"r\") as f:\n",
    "        coarse_params = json.load(f)\n",
    "\n",
    "    print(f\"🔬 Starting FINE tuning for {str_env}...\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"SAC_fine_{str_env}\",\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=123),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, phase=\"fine\", str_env=str_env, coarse_params=coarse_params, save_dir=save_dir),\n",
    "        n_trials=10,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    with open(os.path.join(save_dir, f\"optuna_study_fine.json\"), \"w\") as f:\n",
    "        f.write(study.trials_dataframe().to_json(orient=\"records\", indent=2))\n",
    "\n",
    "\n",
    "run_coarse = False\n",
    "run_fine = False\n",
    "env_list = [\"intersection\", \"racetrack\"]\n",
    "\n",
    "for str_env in env_list:\n",
    "    print(f\"\\n🚦 Running SAC tuning for environment: {str_env}\")\n",
    "    if run_coarse and str_env != \"intersection\":\n",
    "        run_coarse_phase(str_env)\n",
    "    if run_fine:\n",
    "        run_fine_phase(str_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from stable_baselines3 import SAC\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# import gymnasium as gym\n",
    "\n",
    "# SAVE_DIR = \"../trained_models/highway/SAC/\"\n",
    "# # === Load trained Optuna model ===\n",
    "# model_path = os.path.join(SAVE_DIR, \"SAC_best_fine.zip\")\n",
    "# model = SAC.load(model_path)\n",
    "\n",
    "# # === Environment for continued training ===\n",
    "# def make_env(render_mode=None):\n",
    "#     def _init():\n",
    "#         config = {\n",
    "#             \"action\": {\n",
    "#                 \"type\": \"ContinuousAction\"\n",
    "#             },\n",
    "#         }\n",
    "#         env = gym.make(\"highway-fast-v0\", render_mode=render_mode, config=config)\n",
    "#         return Monitor(env)\n",
    "#     return _init\n",
    "\n",
    "\n",
    "# train_env = make_vec_env(make_env(), n_envs=1)\n",
    "\n",
    "# # === Rebind environment in case original wasn't saved in model ===\n",
    "# model.set_env(train_env)\n",
    "\n",
    "# # === Training configuration ===\n",
    "# total_timesteps = 40000\n",
    "# save_interval = 10000\n",
    "# timesteps_run = 0\n",
    "\n",
    "# cp_log_dir = f\"../checkpoints/highway/SAC_optuna\"\n",
    "# os.makedirs(cp_log_dir, exist_ok=True)\n",
    "\n",
    "# while timesteps_run < total_timesteps:\n",
    "#     model.learn(\n",
    "#         total_timesteps=save_interval,\n",
    "#         reset_num_timesteps=False,\n",
    "#         tb_log_name=\"highway_SAC_optuna\",\n",
    "#         log_interval=1,\n",
    "#     )\n",
    "#     timesteps_run += save_interval\n",
    "#     model.save(f\"{cp_log_dir}/{timesteps_run}\")\n",
    "#     print(f\"✅ Saved checkpoint at {timesteps_run} timesteps\")\n",
    "\n",
    "# SAVE_DIR = \"../trained_models/highway/SAC/trained_model_tuned\"\n",
    "# # === Save final model ===\n",
    "# final_model_path = SAVE_DIR\n",
    "# model.save(final_model_path)\n",
    "# print(f\"✅ Final model saved at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating highway environment with SAC...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.04 GiB for an array with shape (1000000, 1, 40, 7) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m str_env \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintersection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mracetrack\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_env\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m environment with SAC...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     vid_path \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Evaluation video saved for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_env\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvid_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mevaluate_env\u001b[1;34m(str_env)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# === Load trained model ===\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../trained_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_env\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/SAC/SAC_best_fine.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m env \u001b[38;5;241m=\u001b[39m make_vec_env(make_env(str_env), n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# === Evaluate and collect frames ===\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\envs\\rl-sac\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:739\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    737\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(data)\n\u001b[0;32m    738\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 739\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# put state_dicts back in place\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_parameters(params, exact_match\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\envs\\rl-sac\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:160\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\envs\\rl-sac\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:189\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass an environment when using `HerReplayBuffer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule,\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs,\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\envs\\rl-sac\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:212\u001b[0m, in \u001b[0;36mReplayBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplayBuffer does not support optimize_memory_usage = True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand handle_timeout_termination = True simultaneously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_memory_usage \u001b[38;5;241m=\u001b[39m optimize_memory_usage\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optimize_memory_usage:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When optimizing memory, `observations` contains also the next observation\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_observations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_shape), dtype\u001b[38;5;241m=\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.04 GiB for an array with shape (1000000, 1, 40, 7) and data type float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from IPython.display import Video\n",
    "\n",
    "\n",
    "def evaluate_env(str_env=None):\n",
    "    # === Configuration ===\n",
    "    algo_name = \"SAC\"\n",
    "    video_eval_dir = f\"../tuned_videos/{str_env}/{algo_name}_tuned/video_eval\"\n",
    "    os.makedirs(video_eval_dir, exist_ok=True)\n",
    "    video_path = os.path.join(video_eval_dir, f\"{algo_name}_eval.mp4\")\n",
    "\n",
    "    # === Load trained model ===\n",
    "    model_path = f\"../trained_models/{str_env}/SAC/SAC_best_fine.zip\"\n",
    "    model = SAC.load(model_path, custom_objects={\"replay_buffer\": None})\n",
    "\n",
    "\n",
    "    env = make_vec_env(make_env(str_env), n_envs=1)\n",
    "    # === Evaluate and collect frames ===\n",
    "    frames = []\n",
    "    num_episodes = 5  # Number of episodes to evaluate\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # === Synchronize the two environments ===\n",
    "        obs_stacked = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs_stacked)\n",
    "            obs_stacked, _, done, _ = env.step(action)\n",
    "\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            if done[0]:  \n",
    "                break\n",
    "\n",
    "        # Add a few idle frames for padding\n",
    "        for _ in range(10):\n",
    "            frames.append(frames[-1])\n",
    "\n",
    "    # === Save video ===\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    # === Display video ===\n",
    "    Video(video_path, embed=True, width=600, height=400)\n",
    "    return video_path\n",
    "\n",
    "for str_env in [\"highway\", \"intersection\", \"racetrack\"]:\n",
    "    print(f\"Evaluating {str_env} environment with SAC...\")\n",
    "    vid_path = evaluate_env(str_env)\n",
    "    print(f\"✅ Evaluation video saved for {str_env} at {vid_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-sac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
